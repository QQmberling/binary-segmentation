{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"binary segmentation tomatos.ipynb","provenance":[{"file_id":"1NL_ao9rAh82cqdW5M69GCfyx9Z0DdNnP","timestamp":1638787579407},{"file_id":"1SrSsnfGGPvNdOtdDjMITfFb_b7rUWw9C","timestamp":1638643010523},{"file_id":"1LO7tAPZQsH1WsDyPFMqTQI7gDs3vIxki","timestamp":1637519493788},{"file_id":"https://github.com/albumentations-team/albumentations_examples/blob/colab/pytorch_semantic_segmentation.ipynb","timestamp":1637427716677}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3d019720","executionInfo":{"status":"ok","timestamp":1639324184845,"user_tz":-180,"elapsed":16943,"user":{"displayName":"Игорь Першин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQIEFUNDOiQOCZx4tVXu6mvrAGWauw5KQxBANh=s64","userId":"03244747994643505397"}},"outputId":"0411cd02-3e22-4f07-9c2c-51bd6d8a43e0"},"source":["!pip install albumentations==0.4.6\n","!pip install ternausnet > /dev/null\n","!pip install segmentation-models-pytorch"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting albumentations==0.4.6\n","  Downloading albumentations-0.4.6.tar.gz (117 kB)\n","\u001b[K     |████████████████████████████████| 117 kB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n","Collecting imgaug>=0.4.0\n","  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n","\u001b[K     |████████████████████████████████| 948 kB 14.4 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.2.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.6)\n","Building wheels for collected packages: albumentations\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65172 sha256=94bb55d62accbc60bcc2e04f72e69a1ef02ac2e97bc16a8e4b994185cb5d10e2\n","  Stored in directory: /root/.cache/pip/wheels/cf/34/0f/cb2a5f93561a181a4bcc84847ad6aaceea8b5a3127469616cc\n","Successfully built albumentations\n","Installing collected packages: imgaug, albumentations\n","  Attempting uninstall: imgaug\n","    Found existing installation: imgaug 0.2.9\n","    Uninstalling imgaug-0.2.9:\n","      Successfully uninstalled imgaug-0.2.9\n","  Attempting uninstall: albumentations\n","    Found existing installation: albumentations 0.1.12\n","    Uninstalling albumentations-0.1.12:\n","      Successfully uninstalled albumentations-0.1.12\n","Successfully installed albumentations-0.4.6 imgaug-0.4.0\n","Collecting segmentation-models-pytorch\n","  Downloading segmentation_models_pytorch-0.2.1-py3-none-any.whl (88 kB)\n","\u001b[K     |████████████████████████████████| 88 kB 3.2 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.11.1+cu111)\n","Collecting efficientnet-pytorch==0.6.3\n","  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n","Collecting pretrainedmodels==0.7.4\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 5.9 MB/s \n","\u001b[?25hCollecting timm==0.4.12\n","  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n","\u001b[K     |████████████████████████████████| 376 kB 24.8 MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (1.10.0+cu111)\n","Collecting munch\n","  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.62.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (3.10.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.19.5)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.15.0)\n","Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12421 sha256=8b6f77f01f207b9fdb21507d5a19ee9ffd236c54266b9210cdce591bba8fd63f\n","  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=3d153801b760f90b07033e32ad6a27f7e349c487d5f5d1eac1ce3f651861276e\n","  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n","Successfully built efficientnet-pytorch pretrainedmodels\n","Installing collected packages: munch, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n","Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.2.1 timm-0.4.12\n"]}]},{"cell_type":"code","metadata":{"id":"edyDFX_Ih5-j"},"source":["from collections import defaultdict\n","import copy\n","import random\n","import os\n","import zipfile\n","import albumentations\n","import albumentations as A\n","import albumentations.augmentations.functional as F\n","import albumentations.augmentations.transforms as G\n","from albumentations.pytorch import ToTensorV2\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import ternausnet.models\n","from tqdm import tqdm\n","import torch\n","import torch.backends.cudnn as cudnn\n","import torch.nn as nn\n","import torch.optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","from google.colab import drive\n","import segmentation_models_pytorch as smp\n","\n","cudnn.benchmark = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gT66r0CVWHJF","executionInfo":{"status":"ok","timestamp":1639324363009,"user_tz":-180,"elapsed":169081,"user":{"displayName":"Игорь Першин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQIEFUNDOiQOCZx4tVXu6mvrAGWauw5KQxBANh=s64","userId":"03244747994643505397"}},"outputId":"f34c7b9a-73b5-4c27-cff7-2d879b2c1615"},"source":["drive.mount('/content/gdrive')\n","IMAGES_DIR = './images'\n","MASKS_DIR = './masks'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ogSJ0OvWHQY","executionInfo":{"status":"ok","timestamp":1639324599291,"user_tz":-180,"elapsed":236284,"user":{"displayName":"Игорь Першин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQIEFUNDOiQOCZx4tVXu6mvrAGWauw5KQxBANh=s64","userId":"03244747994643505397"}},"outputId":"d3373e64-6d65-4829-b520-4bb3dc876bf9"},"source":["DATASET_PATH = '/content/gdrive/My Drive/tomats.zip'\n","zip_object = zipfile.ZipFile(file = DATASET_PATH,mode = 'r')\n","zip_object.extractall(IMAGES_DIR)\n","zip_object.close"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method ZipFile.close of <zipfile.ZipFile filename='/content/gdrive/My Drive/tomats.zip' mode='r'>>"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZEtsTLSHagI7","executionInfo":{"status":"ok","timestamp":1639326882290,"user_tz":-180,"elapsed":338934,"user":{"displayName":"Игорь Першин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQIEFUNDOiQOCZx4tVXu6mvrAGWauw5KQxBANh=s64","userId":"03244747994643505397"}},"outputId":"40de1420-e622-492c-b1ef-a490f84c4fd0"},"source":["from pathlib import Path\n","from multiprocessing import Pool\n","from tqdm import tqdm\n","\n","\n","if not os.path.isdir(MASKS_DIR):\n","  os.mkdir(MASKS_DIR)\n","\n","def make_mask(filename, IMAGES_DIR=IMAGES_DIR, MASKS_DIR=MASKS_DIR):\n","  orginal_img = cv2.imread(os.path.join(IMAGES_DIR, filename))\n","  img = cv2.cvtColor(np.asarray(orginal_img), cv2.COLOR_BGR2HSV)\n","  mask1 = cv2.inRange(img, (0, 20, 20), (80, 255, 255))\n","  mask2 = cv2.inRange(img, (170, 50, 20), (180, 255, 255))\n","  mask = cv2.bitwise_or(mask1, mask2)\n","  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 10))\n","  mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)\n","  mask = mask/255\n","  cv2.imwrite(os.path.join(MASKS_DIR, filename.replace(\".JPG\", \".png\")), mask)\n","\n","\n","filenames = [filename for filename in os.listdir(IMAGES_DIR) if '.JPG' in filename]\n","\n","with Pool(10) as p:\n","    list(tqdm(p.imap(make_mask, filenames), \"Making masks\", total=len(filenames)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Making masks: 100%|██████████| 657/657 [05:37<00:00,  1.95it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"LOJohdL6S1Yw"},"source":["def preprocess_mask(mask):\n","    mask = mask.astype(np.float32)\n","    mask[mask == 2.0] = 0.0\n","    mask[(mask == 1.0) | (mask == 3.0)] = 1.0\n","    return mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u3g0yKpAS1Yw"},"source":["### Define a function to visualize images and their labels"]},{"cell_type":"code","metadata":{"id":"ho-BaziAHY5q"},"source":["def display_image_grid(images_filenames, images_directory, masks_directory, predicted_masks=None):\n","    cols = 3 if predicted_masks else 2\n","    rows = len(images_filenames)\n","    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(10, 24))\n","    for i, image_filename in enumerate(images_filenames):\n","        image = cv2.imread(os.path.join(images_directory, image_filename))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        mask = cv2.imread(os.path.join(masks_directory, image_filename.replace(\".JPG\", \".png\")), cv2.IMREAD_UNCHANGED,)\n","        mask = preprocess_mask(mask)\n","        ax[i, 0].imshow(image)\n","        ax[i, 1].imshow(mask, interpolation=\"nearest\")\n","\n","        ax[i, 0].set_title(\"Image\")\n","        ax[i, 1].set_title(\"Ground truth mask\")\n","\n","        ax[i, 0].set_axis_off()\n","        ax[i, 1].set_axis_off()\n","\n","        if predicted_masks:\n","            predicted_mask = predicted_masks[i]\n","            ax[i, 2].imshow(predicted_mask, interpolation=\"nearest\")\n","            ax[i, 2].set_title(\"Predicted mask\")\n","            ax[i, 2].set_axis_off()\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uxcC6IbgS1Yx"},"source":["display_image_grid(filenames[0:4], IMAGES_DIR, MASKS_DIR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QgLoujrNHY5Z","executionInfo":{"status":"ok","timestamp":1639324966794,"user_tz":-180,"elapsed":897,"user":{"displayName":"Игорь Першин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQIEFUNDOiQOCZx4tVXu6mvrAGWauw5KQxBANh=s64","userId":"03244747994643505397"}},"outputId":"6d1e7314-7bc0-4d58-e197-c3953c46b16c"},"source":["from sklearn.model_selection import train_test_split\n","\n","filenames = sorted(filenames)\n","\n","random.seed(42)\n","random.shuffle(filenames)\n","\n","train_images_filenames, val_images_filenames, y_train, y_val = train_test_split(filenames,\n","                                                    filenames,\n","                                                    test_size = 0.1,\n","                                                    random_state = 42)\n","\n","test_images_filenames = val_images_filenames[:15]\n","\n","print(len(train_images_filenames), len(val_images_filenames), len(test_images_filenames))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["591 66 15\n"]}]},{"cell_type":"markdown","metadata":{"id":"wc8vHhg5S1Yz"},"source":["### Define a PyTorch dataset class"]},{"cell_type":"code","metadata":{"id":"9EtOPkNwh5-8"},"source":["class TomatosDataset(Dataset):\n","    def __init__(self, images_filenames, images_directory, masks_directory, transform=None):\n","        self.images_filenames = images_filenames\n","        self.images_directory = images_directory\n","        self.masks_directory = masks_directory\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images_filenames)\n","\n","    def __getitem__(self, idx):\n","        image_filename = self.images_filenames[idx]\n","        image = cv2.imread(os.path.join(self.images_directory, image_filename))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        mask = cv2.imread(os.path.join(self.masks_directory, image_filename.replace(\".JPG\", \".png\")), cv2.IMREAD_UNCHANGED)\n","        mask = preprocess_mask(mask)\n","        if self.transform is not None:\n","            transformed = self.transform(image=image, mask=mask)\n","            image = transformed[\"image\"]\n","            mask = transformed[\"mask\"]\n","        return image, mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mm_NP-c4h5-_"},"source":["train_transform = A.Compose(\n","    [\n","        A.Resize(256, 256),\n","        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n","        A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n","        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n","        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","        ToTensorV2(),\n","    ]\n",")\n","\n","# train_transform = transforms.Compose([A.Resize(256, 256), transforms.ToTensor()])\n","\n","train_dataset = TomatosDataset(train_images_filenames, IMAGES_DIR, MASKS_DIR, transform=train_transform,)\n","\n","val_transform = A.Compose(\n","    [A.Resize(256, 256), A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()]\n",")\n","val_dataset = TomatosDataset(val_images_filenames, IMAGES_DIR, MASKS_DIR, transform=val_transform,)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3l6giAu5S1Y0"},"source":["Let's define a function that takes a dataset and visualizes different augmentations applied to the same image and the associated mask."]},{"cell_type":"code","metadata":{"id":"FpNuGfJeS1Y0"},"source":["def visualize_augmentations(dataset, idx=0, samples=5):\n","    dataset = copy.deepcopy(dataset)\n","    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n","    figure, ax = plt.subplots(nrows=samples, ncols=2, figsize=(10, 24))\n","    for i in range(samples):\n","        image, mask = dataset[idx]\n","        ax[i, 0].imshow(image)\n","        ax[i, 1].imshow(mask, interpolation=\"nearest\")\n","        ax[i, 0].set_title(\"Augmented image\")\n","        ax[i, 1].set_title(\"Augmented mask\")\n","        ax[i, 0].set_axis_off()\n","        ax[i, 1].set_axis_off()\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fVhtKOq9S1Y1"},"source":["random.seed(42)\n","visualize_augmentations(train_dataset, idx=7)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rKQv-UqhS1Y1"},"source":["### Define helpers for training"]},{"cell_type":"code","metadata":{"id":"oyDZ5iHAYWJN"},"source":["import torch.nn.functional as FF\n","\n","class DiceLossCustom(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLossCustom, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        \n","        #comment out if your model contains a sigmoid or equivalent activation layer\n","        inputs = FF.sigmoid(inputs)       \n","        \n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        \n","        intersection = (inputs * targets).sum()                            \n","        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n","        \n","        return 1 - dice"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zRhPJiCch5_D"},"source":["class MetricMonitor:\n","    def __init__(self, float_precision=4):\n","        self.float_precision = float_precision\n","        self.reset()\n","\n","    def reset(self):\n","        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n","\n","    def update(self, metric_name, val):\n","        metric = self.metrics[metric_name]\n","        \n","        metric[\"val\"] += val\n","        metric[\"count\"] += 1\n","        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n","\n","    def get_value(self, metric_name):\n","        return self.metrics[metric_name][\"avg\"]\n","\n","    def __str__(self):\n","        return \" | \".join(\n","            [\n","                \"{metric_name}: {avg:.{float_precision}f}\".format(\n","                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n","                )\n","                for (metric_name, metric) in self.metrics.items()\n","            ]\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wP7xRhK_S1Y1"},"source":["### Define functions for training and validation"]},{"cell_type":"code","metadata":{"id":"HNz-5F7Bh5_Y"},"source":["def train(train_loader, model, criterion, dice_criterion, optimizer, epoch, params):\n","    metric_monitor = MetricMonitor()\n","    model.train()\n","    stream = tqdm(train_loader)\n","    for i, (images, target) in enumerate(stream, start=1):\n","        images = images.to(params[\"device\"], non_blocking=True)\n","        target = target.to(params[\"device\"], non_blocking=True)\n","        output = model(images).squeeze(1)\n","\n","        loss = criterion(output, target)\n","        metric_monitor.update(\"Loss\", loss.item())\n","\n","        dice = dice_criterion(output, target)\n","        metric_monitor.update(\"DiceLoss\", dice.item())\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        stream.set_description(\n","            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"chKMqryvh5_a"},"source":["def validate(val_loader, model, criterion, dice_criterion, epoch, params):\n","    metric_monitor = MetricMonitor()\n","    model.eval()\n","    stream = tqdm(val_loader)\n","    with torch.no_grad():\n","        for i, (images, target) in enumerate(stream, start=1):\n","            images = images.to(params[\"device\"], non_blocking=True)\n","            target = target.to(params[\"device\"], non_blocking=True)\n","            output = model(images).squeeze(1)\n","\n","            loss = criterion(output, target)\n","            metric_monitor.update(\"Loss\", loss.item())\n","\n","            dice = dice_criterion(output, target)\n","            metric_monitor.update(\"DiceLoss\", dice.item())\n","\n","            stream.set_description(\n","                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n","            )\n","    return metric_monitor.get_value('Loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n4Azt2mNS1Y2"},"source":["def create_model(params):\n","    model = getattr(ternausnet.models, params[\"model\"])(pretrained=True)\n","    model = model.to(params[\"device\"])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tdRUBXHUS1Y2"},"source":["def train_and_validate(model, train_dataset, val_dataset, params):\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=params[\"batch_size\"],\n","        shuffle=True,\n","        num_workers=params[\"num_workers\"],\n","        pin_memory=True,\n","    )\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=params[\"batch_size\"],\n","        shuffle=False,\n","        num_workers=params[\"num_workers\"],\n","        pin_memory=True,\n","    )\n","    criterion = nn.BCEWithLogitsLoss().to(params[\"device\"])\n","    dice_criterion = DiceLossCustom()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n","    min_loss = 10000\n","    for epoch in range(1, params[\"epochs\"] + 1):\n","        train(train_loader, model, criterion, dice_criterion, optimizer, epoch, params)\n","        loss = validate(val_loader, model, criterion, dice_criterion, epoch, params)\n","\n","        if min_loss > loss:\n","            torch.save(model, './best_model.pth')\n","            print(f'Model saved! Now loss={loss}')\n","            min_loss = loss\n","\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cUGoRa3TS1Y3"},"source":["def predict(model, params, test_dataset, batch_size):\n","    test_loader = DataLoader(\n","        test_dataset, batch_size=batch_size, shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True,\n","    )\n","    model.eval()\n","    predictions = []\n","    with torch.no_grad():\n","        for images, (original_heights, original_widths) in test_loader:\n","            images = images.to(params[\"device\"], non_blocking=True)\n","            output = model(images)\n","            probabilities = torch.sigmoid(output.squeeze(1))\n","            predicted_masks = (probabilities >= 0.5).float() * 1\n","            predicted_masks = predicted_masks.cpu().numpy()\n","            for predicted_mask, original_height, original_width in zip(\n","                predicted_masks, original_heights.numpy(), original_widths.numpy()\n","            ):\n","                predictions.append((predicted_mask, original_height, original_width))\n","    return predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wGmcr-HuS1Y3"},"source":["### Define training parameters "]},{"cell_type":"markdown","metadata":{"id":"dNnQOKZcS1Y3"},"source":["Here we define a few training parameters such as model architecture, learning rate, batch size, epochs, etc."]},{"cell_type":"code","metadata":{"id":"uOJZEhwSh5_G"},"source":["params = {\n","    \"model\": \"UNet11\",\n","    \"device\": \"cuda\",\n","    \"lr\": 0.001,\n","    \"batch_size\": 6,\n","    \"num_workers\": 2,\n","    \"epochs\": 10,\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UkfxqSstS1Y4"},"source":["### Train a model"]},{"cell_type":"code","metadata":{"id":"QQ_sSlExGuQ5"},"source":["print(train_dataset[0][0].shape)\n","print(type(train_dataset[0][0]))\n","print(np.unique(train_dataset[0][1]))\n","print(train_dataset[0][1].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IiA5Zgfch5_c"},"source":["model = create_model(params)\n","# model = torch.load('./best_model.pth')\n","model = train_and_validate(model, train_dataset, val_dataset, params)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IounAeb1S1Y5"},"source":["### Predict labels for images and visualize those predictions"]},{"cell_type":"markdown","metadata":{"id":"ZvjqHol1S1Y6"},"source":["Now we have a trained model, so let's try to predict masks for some images. Note that the `__getitem__` method returns not only an image but also the original height and width of an image. We will use those values to resize a predicted mask from the size of 256x256 pixels to the original image's size."]},{"cell_type":"code","metadata":{"id":"RUFvs-g2S1Y6"},"source":["class TomatosInferenceDataset(Dataset):\n","    def __init__(self, images_filenames, images_directory, transform=None):\n","        self.images_filenames = images_filenames\n","        self.images_directory = images_directory\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images_filenames)\n","\n","    def __getitem__(self, idx):\n","        image_filename = self.images_filenames[idx]\n","        image = cv2.imread(os.path.join(self.images_directory, image_filename))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        original_size = tuple(image.shape[:2])\n","        if self.transform is not None:\n","            transformed = self.transform(image=image)\n","            image = transformed[\"image\"]\n","        return image, original_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3hdx06vdS1Y6"},"source":["test_transform = A.Compose(\n","    [A.Resize(256, 256), A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()]\n",")\n","test_dataset = TomatosInferenceDataset(test_images_filenames, IMAGES_DIR, transform=test_transform,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRkC07qnS1Y6"},"source":["predictions = predict(model, params, test_dataset, batch_size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"82eY4ZOtS1Y7"},"source":["predicted_masks = []\n","for predicted_256x256_mask, original_height, original_width in predictions:\n","    full_sized_mask = F.resize(\n","        predicted_256x256_mask, height=original_height, width=original_width, interpolation=cv2.INTER_NEAREST\n","    )\n","    predicted_masks.append(full_sized_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DQnX0p6ES1Y7","scrolled":true},"source":["# display_image_grid(test_images_filenames, IMAGES_DIR, MASKS_DIR, predicted_masks=predicted_masks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IAlu0qlRRfDL"},"source":["def visualize_results(predicted_masks, test_images_filenames=test_images_filenames, samples=3, IMAGES_DIR=IMAGES_DIR, MASKS_DIR=MASKS_DIR):\n","  figure, ax = plt.subplots(nrows=len(predicted_masks[:samples]), ncols=3, figsize=(80, 80))\n","  for i in range(len(predicted_masks[:samples])):\n","    print(os.path.join(IMAGES_DIR, test_images_filenames[i]))\n","    print(os.path.join(MASKS_DIR, test_images_filenames[i].replace(\".JPG\", \".png\")))\n","    img = cv2.imread(os.path.join(IMAGES_DIR, test_images_filenames[i]))\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    real_mask = cv2.imread(os.path.join(MASKS_DIR, test_images_filenames[i].replace(\".JPG\", \".png\")))\n","    ax[i, 0].imshow(img)\n","    ax[i, 1].imshow(real_mask*255, interpolation=\"nearest\")\n","    ax[i, 2].imshow(predicted_masks[i], interpolation=\"nearest\")\n","    ax[i, 0].set_title(\"image\", fontsize=100)\n","    ax[i, 1].set_title(\"real mask\", fontsize=100)\n","    ax[i, 2].set_title(\"predicted mask\", fontsize=100)\n","    ax[i, 0].set_axis_off()\n","    ax[i, 1].set_axis_off()\n","    ax[i, 2].set_axis_off()\n","  plt.tight_layout()\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_contours(predicted_masks, test_images_filenames=test_images_filenames, samples=3, IMAGES_DIR=IMAGES_DIR, MASKS_DIR=MASKS_DIR):\n","  assert len(test_images_filenames)==len(predicted_masks)\n","  figure, ax = plt.subplots(nrows=len(predicted_masks[:samples]), ncols=2, figsize=(80, 80))\n","  for i in range(len(predicted_masks[:samples])):\n","    img = cv2.imread(os.path.join(IMAGES_DIR, test_images_filenames[i]))\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    img_for_realmask = img.copy()\n","    img_for_predmask = img.copy()\n","    real_mask = cv2.imread(os.path.join(MASKS_DIR, test_images_filenames[i].replace(\".JPG\", \".png\")), cv2.IMREAD_GRAYSCALE)\n","    real_contours, hierarchy = cv2.findContours(image=np.uint8(real_mask * 255), mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n","    pred_contours, hierarchy = cv2.findContours(image=np.uint8(predicted_masks[i] * 255), mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n","    cv2.drawContours(image=img_for_realmask, contours=real_contours, contourIdx=-1, color=(0, 255, 0), thickness=16, lineType=cv2.LINE_AA)\n","    cv2.drawContours(image=img_for_predmask, contours=pred_contours, contourIdx=-1, color=(0, 255, 0), thickness=16, lineType=cv2.LINE_AA)\n","    ax[i, 0].imshow(img_for_realmask, interpolation=\"nearest\")\n","    ax[i, 1].imshow(img_for_predmask, interpolation=\"nearest\")\n","    ax[i, 0].set_title(\"image with real contours\", fontsize=100)\n","    ax[i, 1].set_title(\"image with predicted contours\", fontsize=100)\n","    ax[i, 0].set_axis_off()\n","    ax[i, 1].set_axis_off()\n","  plt.tight_layout()\n","  plt.show()\n","\n","\n","# assert len(filenames)==len(predicted_masks)\n","\n","# for i in range(len(filenames[:1])):\n","#   img = cv2.imread(os.path.join(BAD_DIR, filenames[i]))\n","#   img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","#   mask = predicted_masks[i]\n","#   contours, hierarchy = cv2.findContours(image=np.uint8(mask * 255), mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n","#   cv2.drawContours(image=img, contours=contours, contourIdx=-1, color=(0, 255, 0), thickness=2, lineType=cv2.LINE_AA)\n","#   plt.imshow(img)\n","#   plt.subplot()"],"metadata":{"id":"SrwMNPSd3Vks"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-fxN-rbSngIs"},"source":["visualize_results(predicted_masks)\n","  # # contours, hierarchy = cv2.findContours(predicted_masks[i], cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n","  # # _, binarized = cv2.threshold(predicted_masks[i], 125, 255, cv2.THRESH_BINARY)\n","  # # _, contours, hierarchy = cv2.findContours(binarized, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n","  # border = cv2.copyMakeBorder(predicted_masks[i], 1, 1, 1, 1, cv2.BORDER_CONSTANT, value=0 )\n","  # _, contours, hierarchy = cv2.findContours(border, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE, offset=(-1, -1))\n","\n","  # cv2.drawContours(img, contours, -1, (0, 255, 0), 3)\n","  # plt.imshow(img)\n","  # plt.figure()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P55839oK70NX","executionInfo":{"status":"ok","timestamp":1639327051908,"user_tz":-180,"elapsed":1072,"user":{"displayName":"Игорь Першин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQIEFUNDOiQOCZx4tVXu6mvrAGWauw5KQxBANh=s64","userId":"03244747994643505397"}},"outputId":"8bacdc4d-ed92-4dea-96b8-a8325143a543"},"source":["BAD_DIR = './bad'\n","BAD_MASK_DIR = './bad_mask'\n","\n","DATASET_PATH = '/content/gdrive/My Drive/bad_tomats.zip'\n","zip_object = zipfile.ZipFile(file = DATASET_PATH,mode = 'r')\n","zip_object.extractall(BAD_DIR)\n","zip_object.close"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method ZipFile.close of <zipfile.ZipFile filename='/content/gdrive/My Drive/bad_tomats.zip' mode='r'>>"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HFIbB7Ao8AqC","executionInfo":{"status":"ok","timestamp":1639327058474,"user_tz":-180,"elapsed":4909,"user":{"displayName":"Игорь Першин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQIEFUNDOiQOCZx4tVXu6mvrAGWauw5KQxBANh=s64","userId":"03244747994643505397"}},"outputId":"13b8ca6d-c3ad-4ca5-a6cc-46471e4a6f69"},"source":["from functools import partial\n","\n","if not os.path.isdir(BAD_MASK_DIR):\n","  os.mkdir(BAD_MASK_DIR)\n","\n","filenames = os.listdir(BAD_DIR)\n","\n","with Pool(10) as p:\n","  list(tqdm(p.imap(partial(make_mask, IMAGES_DIR=BAD_DIR, MASKS_DIR=BAD_MASK_DIR), filenames), \"Making masks\", total=len(filenames)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Making masks: 100%|██████████| 6/6 [00:03<00:00,  1.68it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"TwaXy4fmAP5d"},"source":["test_transform = A.Compose(\n","    [A.Resize(256, 256), A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()]\n",")\n","test_dataset = TomatosInferenceDataset(filenames, BAD_DIR, transform=test_transform,)\n","predictions = predict(model, params, test_dataset, batch_size=1)\n","predicted_masks = []\n","for predicted_256x256_mask, original_height, original_width in predictions:\n","    full_sized_mask = F.resize(\n","        predicted_256x256_mask, height=original_height, width=original_width, interpolation=cv2.INTER_NEAREST\n","    )\n","    predicted_masks.append(full_sized_mask)\n","visualize_results(predicted_masks, test_images_filenames=filenames, samples=6, IMAGES_DIR=BAD_DIR, MASKS_DIR=BAD_MASK_DIR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S1ksm1k-ILAX"},"source":["torch.save(model, '/content/gdrive/MyDrive/best_model.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = torch.load('/content/gdrive/MyDrive/best_model.pth')"],"metadata":{"id":"ENr3B4-HYsbt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_contours(predicted_masks, test_images_filenames=filenames, samples=6, IMAGES_DIR=BAD_DIR, MASKS_DIR=BAD_MASK_DIR)"],"metadata":{"id":"K22-G9hD5Jy7"},"execution_count":null,"outputs":[]}]}